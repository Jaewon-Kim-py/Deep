{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1) 알콜(Alcohol)\n",
    "  2) 말산(Malic acid)\n",
    "  3) 회분(Ash)\n",
    "  4) 회분의 알칼리도(Alcalinity of ash)\n",
    "  5) 마그네슘(Magnesium)\n",
    "  6) 총 폴리페놀(Total phenols)\n",
    "  7) 플라보노이드 폴리페놀(Flavanoids)\n",
    "  8) 비 플라보노이드 폴리페놀(Nonflavanoid phenols)\n",
    "  9) 프로안토시아닌(Proanthocyanins)\n",
    "  10) 색상의 강도(Color intensity)\n",
    "  11) 색상(Hue)\n",
    "  12) 희석 와인의 OD280/OD315 비율 (OD280/OD315 of diluted wines)\n",
    "  13) 프롤린(Proline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "datasets = load_wine()\n",
    "\n",
    "X = datasets.data\n",
    "\n",
    "y = datasets.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline class  \n",
       "0                          3.92   1065.0     0  \n",
       "1                          3.40   1050.0     0  \n",
       "2                          3.17   1185.0     0  \n",
       "3                          3.45   1480.0     0  \n",
       "4                          2.93    735.0     0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(datasets.data, columns=datasets.feature_names)\n",
    "target = pd.Series(datasets.target, dtype=\"category\")\n",
    "df['class'] = target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.3,random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규화 및 tensor로 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "\n",
    "standardScaler.fit(X_train)\n",
    "\n",
    "X_scaling_train = standardScaler.transform(X_train)\n",
    "X_scaling_test = standardScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "X_train_tensor = torch.tensor(X_scaling_train, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_scaling_test, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        #super(LogisticRegression, self).__init__()\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        #self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = self.linear(X)\n",
    "    \n",
    "        return output\n",
    "\n",
    "model = LogisticRegression(13, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function, optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/20000] Loss:1.202 Acc: 31.452\n",
      "Epoch[100/20000] Loss:1.016 Acc: 45.968\n",
      "Epoch[200/20000] Loss:0.873 Acc: 65.323\n",
      "Epoch[300/20000] Loss:0.763 Acc: 76.613\n",
      "Epoch[400/20000] Loss:0.679 Acc: 84.677\n",
      "Epoch[500/20000] Loss:0.612 Acc: 85.484\n",
      "Epoch[600/20000] Loss:0.559 Acc: 87.903\n",
      "Epoch[700/20000] Loss:0.516 Acc: 89.516\n",
      "Epoch[800/20000] Loss:0.48 Acc: 90.323\n",
      "Epoch[900/20000] Loss:0.449 Acc: 91.129\n",
      "Epoch[1000/20000] Loss:0.423 Acc: 91.935\n",
      "Epoch[1100/20000] Loss:0.401 Acc: 93.548\n",
      "Epoch[1200/20000] Loss:0.381 Acc: 94.355\n",
      "Epoch[1300/20000] Loss:0.364 Acc: 94.355\n",
      "Epoch[1400/20000] Loss:0.348 Acc: 94.355\n",
      "Epoch[1500/20000] Loss:0.334 Acc: 94.355\n",
      "Epoch[1600/20000] Loss:0.322 Acc: 94.355\n",
      "Epoch[1700/20000] Loss:0.31 Acc: 94.355\n",
      "Epoch[1800/20000] Loss:0.3 Acc: 94.355\n",
      "Epoch[1900/20000] Loss:0.29 Acc: 94.355\n",
      "Epoch[2000/20000] Loss:0.282 Acc: 94.355\n",
      "Epoch[2100/20000] Loss:0.274 Acc: 94.355\n",
      "Epoch[2200/20000] Loss:0.266 Acc: 94.355\n",
      "Epoch[2300/20000] Loss:0.259 Acc: 94.355\n",
      "Epoch[2400/20000] Loss:0.252 Acc: 94.355\n",
      "Epoch[2500/20000] Loss:0.246 Acc: 94.355\n",
      "Epoch[2600/20000] Loss:0.24 Acc: 94.355\n",
      "Epoch[2700/20000] Loss:0.235 Acc: 95.161\n",
      "Epoch[2800/20000] Loss:0.23 Acc: 95.161\n",
      "Epoch[2900/20000] Loss:0.225 Acc: 95.161\n",
      "Epoch[3000/20000] Loss:0.22 Acc: 95.161\n",
      "Epoch[3100/20000] Loss:0.216 Acc: 95.161\n",
      "Epoch[3200/20000] Loss:0.212 Acc: 95.161\n",
      "Epoch[3300/20000] Loss:0.208 Acc: 95.161\n",
      "Epoch[3400/20000] Loss:0.204 Acc: 95.968\n",
      "Epoch[3500/20000] Loss:0.201 Acc: 95.968\n",
      "Epoch[3600/20000] Loss:0.197 Acc: 95.968\n",
      "Epoch[3700/20000] Loss:0.194 Acc: 96.774\n",
      "Epoch[3800/20000] Loss:0.191 Acc: 96.774\n",
      "Epoch[3900/20000] Loss:0.188 Acc: 96.774\n",
      "Epoch[4000/20000] Loss:0.185 Acc: 96.774\n",
      "Epoch[4100/20000] Loss:0.182 Acc: 96.774\n",
      "Epoch[4200/20000] Loss:0.18 Acc: 96.774\n",
      "Epoch[4300/20000] Loss:0.177 Acc: 96.774\n",
      "Epoch[4400/20000] Loss:0.175 Acc: 96.774\n",
      "Epoch[4500/20000] Loss:0.172 Acc: 97.581\n",
      "Epoch[4600/20000] Loss:0.17 Acc: 97.581\n",
      "Epoch[4700/20000] Loss:0.168 Acc: 97.581\n",
      "Epoch[4800/20000] Loss:0.166 Acc: 97.581\n",
      "Epoch[4900/20000] Loss:0.164 Acc: 97.581\n",
      "Epoch[5000/20000] Loss:0.162 Acc: 97.581\n",
      "Epoch[5100/20000] Loss:0.16 Acc: 98.387\n",
      "Epoch[5200/20000] Loss:0.158 Acc: 98.387\n",
      "Epoch[5300/20000] Loss:0.156 Acc: 98.387\n",
      "Epoch[5400/20000] Loss:0.154 Acc: 98.387\n",
      "Epoch[5500/20000] Loss:0.152 Acc: 98.387\n",
      "Epoch[5600/20000] Loss:0.151 Acc: 98.387\n",
      "Epoch[5700/20000] Loss:0.149 Acc: 98.387\n",
      "Epoch[5800/20000] Loss:0.148 Acc: 98.387\n",
      "Epoch[5900/20000] Loss:0.146 Acc: 98.387\n",
      "Epoch[6000/20000] Loss:0.145 Acc: 98.387\n",
      "Epoch[6100/20000] Loss:0.143 Acc: 98.387\n",
      "Epoch[6200/20000] Loss:0.142 Acc: 98.387\n",
      "Epoch[6300/20000] Loss:0.14 Acc: 98.387\n",
      "Epoch[6400/20000] Loss:0.139 Acc: 98.387\n",
      "Epoch[6500/20000] Loss:0.138 Acc: 98.387\n",
      "Epoch[6600/20000] Loss:0.136 Acc: 98.387\n",
      "Epoch[6700/20000] Loss:0.135 Acc: 98.387\n",
      "Epoch[6800/20000] Loss:0.134 Acc: 98.387\n",
      "Epoch[6900/20000] Loss:0.133 Acc: 98.387\n",
      "Epoch[7000/20000] Loss:0.132 Acc: 98.387\n",
      "Epoch[7100/20000] Loss:0.13 Acc: 99.194\n",
      "Epoch[7200/20000] Loss:0.129 Acc: 99.194\n",
      "Epoch[7300/20000] Loss:0.128 Acc: 99.194\n",
      "Epoch[7400/20000] Loss:0.127 Acc: 99.194\n",
      "Epoch[7500/20000] Loss:0.126 Acc: 99.194\n",
      "Epoch[7600/20000] Loss:0.125 Acc: 99.194\n",
      "Epoch[7700/20000] Loss:0.124 Acc: 99.194\n",
      "Epoch[7800/20000] Loss:0.123 Acc: 99.194\n",
      "Epoch[7900/20000] Loss:0.122 Acc: 99.194\n",
      "Epoch[8000/20000] Loss:0.121 Acc: 99.194\n",
      "Epoch[8100/20000] Loss:0.12 Acc: 99.194\n",
      "Epoch[8200/20000] Loss:0.12 Acc: 99.194\n",
      "Epoch[8300/20000] Loss:0.119 Acc: 99.194\n",
      "Epoch[8400/20000] Loss:0.118 Acc: 99.194\n",
      "Epoch[8500/20000] Loss:0.117 Acc: 99.194\n",
      "Epoch[8600/20000] Loss:0.116 Acc: 99.194\n",
      "Epoch[8700/20000] Loss:0.115 Acc: 99.194\n",
      "Epoch[8800/20000] Loss:0.114 Acc: 99.194\n",
      "Epoch[8900/20000] Loss:0.114 Acc: 99.194\n",
      "Epoch[9000/20000] Loss:0.113 Acc: 99.194\n",
      "Epoch[9100/20000] Loss:0.112 Acc: 99.194\n",
      "Epoch[9200/20000] Loss:0.111 Acc: 99.194\n",
      "Epoch[9300/20000] Loss:0.111 Acc: 99.194\n",
      "Epoch[9400/20000] Loss:0.11 Acc: 99.194\n",
      "Epoch[9500/20000] Loss:0.109 Acc: 99.194\n",
      "Epoch[9600/20000] Loss:0.109 Acc: 99.194\n",
      "Epoch[9700/20000] Loss:0.108 Acc: 99.194\n",
      "Epoch[9800/20000] Loss:0.107 Acc: 99.194\n",
      "Epoch[9900/20000] Loss:0.106 Acc: 99.194\n",
      "Epoch[10000/20000] Loss:0.106 Acc: 99.194\n",
      "Epoch[10100/20000] Loss:0.105 Acc: 99.194\n",
      "Epoch[10200/20000] Loss:0.105 Acc: 99.194\n",
      "Epoch[10300/20000] Loss:0.104 Acc: 99.194\n",
      "Epoch[10400/20000] Loss:0.103 Acc: 99.194\n",
      "Epoch[10500/20000] Loss:0.103 Acc: 99.194\n",
      "Epoch[10600/20000] Loss:0.102 Acc: 99.194\n",
      "Epoch[10700/20000] Loss:0.102 Acc: 99.194\n",
      "Epoch[10800/20000] Loss:0.101 Acc: 99.194\n",
      "Epoch[10900/20000] Loss:0.1 Acc: 99.194\n",
      "Epoch[11000/20000] Loss:0.1 Acc: 99.194\n",
      "Epoch[11100/20000] Loss:0.099 Acc: 99.194\n",
      "Epoch[11200/20000] Loss:0.099 Acc: 99.194\n",
      "Epoch[11300/20000] Loss:0.098 Acc: 99.194\n",
      "Epoch[11400/20000] Loss:0.098 Acc: 99.194\n",
      "Epoch[11500/20000] Loss:0.097 Acc: 99.194\n",
      "Epoch[11600/20000] Loss:0.097 Acc: 99.194\n",
      "Epoch[11700/20000] Loss:0.096 Acc: 99.194\n",
      "Epoch[11800/20000] Loss:0.096 Acc: 99.194\n",
      "Epoch[11900/20000] Loss:0.095 Acc: 99.194\n",
      "Epoch[12000/20000] Loss:0.095 Acc: 99.194\n",
      "Epoch[12100/20000] Loss:0.094 Acc: 99.194\n",
      "Epoch[12200/20000] Loss:0.094 Acc: 99.194\n",
      "Epoch[12300/20000] Loss:0.093 Acc: 99.194\n",
      "Epoch[12400/20000] Loss:0.093 Acc: 99.194\n",
      "Epoch[12500/20000] Loss:0.092 Acc: 99.194\n",
      "Epoch[12600/20000] Loss:0.092 Acc: 99.194\n",
      "Epoch[12700/20000] Loss:0.091 Acc: 99.194\n",
      "Epoch[12800/20000] Loss:0.091 Acc: 99.194\n",
      "Epoch[12900/20000] Loss:0.09 Acc: 99.194\n",
      "Epoch[13000/20000] Loss:0.09 Acc: 99.194\n",
      "Epoch[13100/20000] Loss:0.09 Acc: 99.194\n",
      "Epoch[13200/20000] Loss:0.089 Acc: 99.194\n",
      "Epoch[13300/20000] Loss:0.089 Acc: 99.194\n",
      "Epoch[13400/20000] Loss:0.088 Acc: 99.194\n",
      "Epoch[13500/20000] Loss:0.088 Acc: 99.194\n",
      "Epoch[13600/20000] Loss:0.088 Acc: 99.194\n",
      "Epoch[13700/20000] Loss:0.087 Acc: 99.194\n",
      "Epoch[13800/20000] Loss:0.087 Acc: 99.194\n",
      "Epoch[13900/20000] Loss:0.086 Acc: 99.194\n",
      "Epoch[14000/20000] Loss:0.086 Acc: 99.194\n",
      "Epoch[14100/20000] Loss:0.086 Acc: 99.194\n",
      "Epoch[14200/20000] Loss:0.085 Acc: 99.194\n",
      "Epoch[14300/20000] Loss:0.085 Acc: 99.194\n",
      "Epoch[14400/20000] Loss:0.084 Acc: 99.194\n",
      "Epoch[14500/20000] Loss:0.084 Acc: 99.194\n",
      "Epoch[14600/20000] Loss:0.084 Acc: 99.194\n",
      "Epoch[14700/20000] Loss:0.083 Acc: 99.194\n",
      "Epoch[14800/20000] Loss:0.083 Acc: 99.194\n",
      "Epoch[14900/20000] Loss:0.083 Acc: 99.194\n",
      "Epoch[15000/20000] Loss:0.082 Acc: 99.194\n",
      "Epoch[15100/20000] Loss:0.082 Acc: 99.194\n",
      "Epoch[15200/20000] Loss:0.082 Acc: 99.194\n",
      "Epoch[15300/20000] Loss:0.081 Acc: 99.194\n",
      "Epoch[15400/20000] Loss:0.081 Acc: 99.194\n",
      "Epoch[15500/20000] Loss:0.081 Acc: 99.194\n",
      "Epoch[15600/20000] Loss:0.08 Acc: 99.194\n",
      "Epoch[15700/20000] Loss:0.08 Acc: 99.194\n",
      "Epoch[15800/20000] Loss:0.08 Acc: 99.194\n",
      "Epoch[15900/20000] Loss:0.079 Acc: 99.194\n",
      "Epoch[16000/20000] Loss:0.079 Acc: 99.194\n",
      "Epoch[16100/20000] Loss:0.079 Acc: 99.194\n",
      "Epoch[16200/20000] Loss:0.078 Acc: 99.194\n",
      "Epoch[16300/20000] Loss:0.078 Acc: 99.194\n",
      "Epoch[16400/20000] Loss:0.078 Acc: 99.194\n",
      "Epoch[16500/20000] Loss:0.077 Acc: 99.194\n",
      "Epoch[16600/20000] Loss:0.077 Acc: 99.194\n",
      "Epoch[16700/20000] Loss:0.077 Acc: 99.194\n",
      "Epoch[16800/20000] Loss:0.077 Acc: 99.194\n",
      "Epoch[16900/20000] Loss:0.076 Acc: 99.194\n",
      "Epoch[17000/20000] Loss:0.076 Acc: 99.194\n",
      "Epoch[17100/20000] Loss:0.076 Acc: 99.194\n",
      "Epoch[17200/20000] Loss:0.075 Acc: 99.194\n",
      "Epoch[17300/20000] Loss:0.075 Acc: 99.194\n",
      "Epoch[17400/20000] Loss:0.075 Acc: 99.194\n",
      "Epoch[17500/20000] Loss:0.075 Acc: 99.194\n",
      "Epoch[17600/20000] Loss:0.074 Acc: 99.194\n",
      "Epoch[17700/20000] Loss:0.074 Acc: 99.194\n",
      "Epoch[17800/20000] Loss:0.074 Acc: 99.194\n",
      "Epoch[17900/20000] Loss:0.074 Acc: 99.194\n",
      "Epoch[18000/20000] Loss:0.073 Acc: 99.194\n",
      "Epoch[18100/20000] Loss:0.073 Acc: 99.194\n",
      "Epoch[18200/20000] Loss:0.073 Acc: 99.194\n",
      "Epoch[18300/20000] Loss:0.073 Acc: 99.194\n",
      "Epoch[18400/20000] Loss:0.072 Acc: 99.194\n",
      "Epoch[18500/20000] Loss:0.072 Acc: 99.194\n",
      "Epoch[18600/20000] Loss:0.072 Acc: 99.194\n",
      "Epoch[18700/20000] Loss:0.072 Acc: 99.194\n",
      "Epoch[18800/20000] Loss:0.071 Acc: 99.194\n",
      "Epoch[18900/20000] Loss:0.071 Acc: 99.194\n",
      "Epoch[19000/20000] Loss:0.071 Acc: 99.194\n",
      "Epoch[19100/20000] Loss:0.071 Acc: 99.194\n",
      "Epoch[19200/20000] Loss:0.07 Acc: 99.194\n",
      "Epoch[19300/20000] Loss:0.07 Acc: 99.194\n",
      "Epoch[19400/20000] Loss:0.07 Acc: 99.194\n",
      "Epoch[19500/20000] Loss:0.07 Acc: 99.194\n",
      "Epoch[19600/20000] Loss:0.069 Acc: 99.194\n",
      "Epoch[19700/20000] Loss:0.069 Acc: 99.194\n",
      "Epoch[19800/20000] Loss:0.069 Acc: 99.194\n",
      "Epoch[19900/20000] Loss:0.069 Acc: 99.194\n"
     ]
    }
   ],
   "source": [
    "epochs = 20000\n",
    "total = 0\n",
    "correct = 0\n",
    "#model.train()\n",
    "for epoch in range(epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    prediction = model(X_train_tensor)\n",
    "   \n",
    "    loss = criterion(prediction, y_train_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    _, predicted = torch.max(prediction.data, 1)\n",
    "    total += y_train_tensor.size(0)\n",
    "    correct += (predicted == y_train_tensor).sum().item()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch[{epoch}/{epochs}] Loss:{round(loss.item(), 3)} Acc: {round(100 * correct / total, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([3.7103, 2.0904, 4.2079, 0.7342, 3.9097, 4.2466, 3.2568, 3.1780, 4.2295,\n",
       "        4.3813, 2.2345, 3.2139, 3.7977, 2.7289, 3.5194, 2.8019, 2.9252, 3.6527,\n",
       "        1.5284, 3.5443, 5.3087, 3.1315, 2.7272, 1.8528, 4.7221, 2.4035, 4.6492,\n",
       "        4.9445, 4.7683, 4.8060, 1.5452, 3.8336, 3.2641, 2.2130, 5.5157, 4.9782,\n",
       "        3.8592, 2.3231, 2.1094, 3.4066, 4.5526, 3.9933, 3.5826, 3.6945, 4.2736,\n",
       "        3.2645, 1.6821, 4.1212, 3.6881, 3.8922, 1.5725, 4.6310, 1.8523, 3.5894,\n",
       "        3.1664, 4.4531, 2.4075, 4.6600, 1.0562, 3.6785, 1.8466, 5.2380, 2.3911,\n",
       "        2.6928, 4.1068, 3.0701, 2.9748, 3.1946, 4.2743, 4.0469, 1.6309, 4.9424,\n",
       "        5.3694, 3.4625, 4.4675, 6.5779, 6.0969, 2.8912, 2.8575, 2.0587, 2.1490,\n",
       "        2.2986, 4.5548, 1.4366, 3.4005, 4.3949, 3.1571, 2.6544, 3.4861, 4.2089,\n",
       "        2.7751, 1.6108, 4.4027, 4.0729, 2.9676, 1.6350, 1.4714, 2.2270, 4.0007,\n",
       "        3.4821, 2.6513, 1.4111, 4.1274, 3.8307, 3.8960, 3.6448, 4.6778, 2.4753,\n",
       "        3.5453, 2.9436, 1.8360, 3.3961, 3.3441, 3.5385, 2.9673, 4.0247, 2.0072,\n",
       "        2.5914, 4.0059, 3.9861, 1.6043, 3.5454, 3.2705, 2.3582]),\n",
       "indices=tensor([2, 1, 1, 0, 2, 0, 0, 2, 2, 2, 1, 1, 0, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 2,\n",
       "        1, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 2, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0,\n",
       "        1, 0, 0, 1, 2, 1, 0, 2, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 2, 0,\n",
       "        1, 2, 2, 0, 1, 0, 1, 1, 1, 1, 1, 2, 2, 0, 2, 2, 0, 2, 1, 2, 0, 1, 0, 0,\n",
       "        1, 1, 2, 1, 1, 0, 2, 2, 2, 2, 1, 1, 1, 0, 2, 0, 2, 2, 0, 2, 1, 2, 1, 1,\n",
       "        1, 1, 2, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#궁금한 내용\n",
    "torch.max(prediction.data,1) #values 와 indices가 출력됨. 원하는 결과 위해서는 _(앞내용),predicted(필요한 내용)으로 처리! / #1=차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.19354838709677\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(prediction.data, 1)\n",
    "\n",
    "total += y_train_tensor.size(0)\n",
    "correct += (predicted == y_train_tensor).sum().item()\n",
    "\n",
    "acc = 100 * correct / total\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 100.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "total = 0\n",
    "correct = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    prediction = model(X_test_tensor)\n",
    "    _, predicted = torch.max(prediction.data, 1)\n",
    "    total += y_test_tensor.size(0)\n",
    "    correct += (predicted == y_test_tensor).sum().item()\n",
    "    \n",
    "    print(f'Test Acc: {round(100 * correct / total, 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
